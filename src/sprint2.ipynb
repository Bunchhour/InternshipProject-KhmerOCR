{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806166dd",
   "metadata": {},
   "source": [
    "**üìÇ Step 1: Create src/model.py (The Brain)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abde22",
   "metadata": {},
   "source": [
    "We will build a simple CNN. This network looks at the image and extracts features.\n",
    "\n",
    "**Crucial Logic:**\n",
    "\n",
    "**Input:** Image ```(Batch, 1, 32, Width)```\n",
    "\n",
    "**CNN:** Squashes the height from ```32``` down to ```1```.\n",
    "\n",
    "**Permute:** Swaps dimensions so ``Width`` becomes ``Time``. CTC needs ``(Time, Batch, Classes)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915b2ad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Import your modules\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KhmerOCRDataset \u001b[38;5;66;03m#‚ö†Ô∏è Need to check\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KhmerLabelConverter \u001b[38;5;66;03m# ‚ö†Ô∏èNeed to check\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleOCR\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src.dataset'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your modules\n",
    "from src.dataset import KhmerOCRDataset #‚ö†Ô∏è Need to check\n",
    "from src.utils import KhmerLabelConverter # ‚ö†Ô∏èNeed to check\n",
    "from src.model import SimpleOCR\n",
    "from datasets import load_dataset\n",
    "\n",
    "def train():\n",
    "    # --- COFIG ---\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCH = 5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Training on {DEVICE}....\")\n",
    "\n",
    "    # 1. Setup Data\n",
    "    # Load raw data to build vocab\n",
    "    raw_data = load_dataset(\"seanghay/khmer-hanuman-100k\", split=\"train[:2000]\") # Small subset for Sprint 2\n",
    "    all_text = \"\".join([x['text'] for x in raw_data])\n",
    "    vocab = sorted(list(set(all_text)))\n",
    "    \"\"\" \n",
    "    What happens here:\n",
    "\n",
    "    1. Collect all Khmer characters\n",
    "    2. Remove duplicates\n",
    "    3. Sort characters\n",
    "    \"\"\"\n",
    "    converter = KhmerLabelConverter(vocab)\n",
    "    train_dataset = KhmerOCRDataset(split=\"train[:2000]\", converter=converter)\n",
    "\n",
    "    # Collate function handles variable width images\n",
    "    def collate_fn(batch):\n",
    "        images = [item['image'] for item in batch]\n",
    "        labels = [item['label'] for item in batch]\n",
    "        original_texts = [item['original_text'] for item in batch]\n",
    "        label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "        # Pad images to the widest in the batch\n",
    "        max_w = max([img.shape[2] for img in images])\n",
    "        padded_imgs = torch.zeros(len(images), 1,32, max_w)\n",
    "        for i, img in enumerate(images):\n",
    "            w = img.shape[2]\n",
    "            padded_imgs[i, :, :, :w] = img\n",
    "\n",
    "        # Flatten labels for CTC\n",
    "        labels_concat = torch.cat(labels)\n",
    "        return padded_imgs, labels_concat, label_lengths, original_texts\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # 2. setup Model\n",
    "    model = SimpleOCR(num_classes=converter.get_num_classes()).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    criteration = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "\n",
    "        for images, targets, target_lengths, origainal_texts in pbar:\n",
    "            images = images.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # Forward Pass\n",
    "            # Preds shape: (Time, Batch, NumClasses)\n",
    "            preds = model(images)\n",
    "\n",
    "            # Calculate Input Lengths (Time steps)\n",
    "            # CNN reduces width by 8x (2*2*2). So Time = Width // 8\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), fill_value=preds.size(0), dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criteration(preds, targets, input_lengths, target_lengths)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        # --- QUICK TEST (Reality Check) ---\n",
    "        # Decode one prediction to see if it's learning\n",
    "        with torch.no_grad():\n",
    "            # Greedy Decode: Take max probability at each step\n",
    "            _, max_index = torch.max(preds, dim=2) # (Time, Batch)\n",
    "            pred_indices = max_index[:, 0].cpu().numpy().tolist() # Take first item in batch\n",
    "            decoded_text = converter.decode(pred_indices)\n",
    "            \n",
    "            print(f\"\\n--- Reality Check (Epoch {epoch+1}) ---\")\n",
    "            print(f\"Target: {original_texts[0]}\")\n",
    "            print(f\"Pred:   {decoded_text}\")\n",
    "            print(f\"--------------------------------------\\n\")\n",
    "    # Save Model\n",
    "    torch.save(model.state_dict(), \"checkpoints/sprint2_model.pth\")\n",
    "    print(\"‚úÖ Model Saved!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
